{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful tool for tracking the total time it takes to run the code on varied machines\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages used throughout the code\n",
    "# This is a complete list of all packages that need to be imported for use throughout the code\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import gc\n",
    "import string\n",
    "import holidays\n",
    "import re\n",
    "from haversine import haversine, Unit\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be provided by Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset with the number of meals per meal code per cabin per flight\n",
    "# Source: Delta\n",
    "mealsPerFlight_historic = pd.read_csv('data/insert_path_to_mealsPerFlight_historic.csv', header=0)\n",
    "\n",
    "# Passenger Load Data for retrieving flight times\n",
    "passengerLoadData_historic = pd.read_csv(\"data/insert_path_to_passenger_load_data_historic.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of flights you want to predict on\n",
    "# Requires 1 row for each flight\n",
    "# Each row requires the following features:\n",
    "# 'DepartTime', 'FlightDate', 'FlightOrigin', 'FlightDestination', 'FlightNumber', 'AircraftType'\n",
    "input_dataset = pd.read_csv('data/insert_path_to_input_data.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets provided by MSBA team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dataset that contains capacity information regarding \n",
    "# Source: Extracted data from Delta PDF (extraction by Pamela Cheng)\n",
    "capacity_info = pd.read_excel('data/Fleet Key_v1.xlsx', header=0)\n",
    "\n",
    "# Additional dataset that associate airport IATA codes with their country and coordinates\n",
    "# Source: https://github.com/datasets/airport-codes/blob/master/data/airport-codes.csv\n",
    "airport_codes = pd.read_csv(\"data/airport-codes.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mealsPerFlight_historic\n",
    "This is the main dataset that we are using for the model, mealsPerFlight_historic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure FlightDate is formatted as datetime in the df\n",
    "mealsPerFlight_historic['FlightDate'] = pd.to_datetime(mealsPerFlight_historic['FlightDate'], errors='coerce')\n",
    "\n",
    "# Strip whitespace from string columns for merging later\n",
    "mealsPerFlight_historic['FlightOrigin'] = mealsPerFlight_historic['FlightOrigin'].str.strip()\n",
    "mealsPerFlight_historic['FlightDestination'] = mealsPerFlight_historic['FlightDestination'].str.strip()\n",
    "mealsPerFlight_historic['EntreeType'] = mealsPerFlight_historic['EntreeType'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure FlightDate is formatted as datetime in the df\n",
    "input_dataset['FlightDate'] = pd.to_datetime(input_dataset['FlightDate'], errors='coerce')\n",
    "\n",
    "# Strip whitespace from string columns for merging later\n",
    "input_dataset['FlightOrigin'] = input_dataset['FlightOrigin'].str.strip()\n",
    "input_dataset['FlightDestination'] = input_dataset['FlightDestination'].str.strip()\n",
    "input_dataset['EntreeType'] = input_dataset['EntreeType'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## passengerLoadData_historic\n",
    "This is the passenger load data provided by Delta, which contains information on the takeoff time at the origin, as well as the capacity for each flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "passengerLoadData_historic['Flt_Orig_Dt'] = pd.to_datetime(passengerLoadData_historic['Flt_Orig_Dt'])\n",
    "passengerLoadData_historic['Schd_Dprt_LTs'] = pd.to_datetime(passengerLoadData_historic['Schd_Dprt_LTs'])\n",
    "passengerLoadData_historic['Schd_Arr_LTs'] = pd.to_datetime(passengerLoadData_historic['Schd_Arr_LTs'])\n",
    "passengerLoadData_historic.drop(columns=['Index', 'Schd_Arr_LTs'], inplace=True)\n",
    "\n",
    "# Rename columns for easier processing, merging later\n",
    "flight_times = passengerLoadData_historic.rename(columns={\n",
    "    'Flt_Orig_Dt': 'FlightDate',\n",
    "    'Flt_Nb': 'FlightNumber',\n",
    "    'Schd_Orig_Stn_Cd': 'FlightOrigin',\n",
    "    'Schd_Dest_Stn_Cd': 'FlightDestination',\n",
    "    'Actl_Ac_Typ_Cd': 'AircraftType',\n",
    "    'Dom_Intl_Cd': 'DomOrInt',\n",
    "    'Schd_Dprt_LTs': 'DepartTime',\n",
    "    'Pax_Ct': 'PassengerCount',\n",
    "    'Seat_Avby_Ct': 'PassengerCapacity'\n",
    "})\n",
    "\n",
    "# Create new columns based on the values in 'DomOrInt'\n",
    "# Can use these as features later to count the number of domestic and international flights in an aggregation\n",
    "# flight_times['IsDom'] = (flight_times['DomOrInt'] == 'D').astype(int) # is a domestic flight\n",
    "# flight_times['IsInt'] = (flight_times['DomOrInt'] == 'I').astype(int) # is an international flight\n",
    "# flight_times.drop(columns=['DomOrInt', 'PassengerCount'], inplace=True) # Drop the original 'DomOrInt' column\n",
    "\n",
    "# Strip whitespace from FlightOrigin and FlightDestination -- important for merges later\n",
    "flight_times['FlightOrigin'] = flight_times['FlightOrigin'].str.strip()\n",
    "flight_times['FlightDestination'] = flight_times['FlightDestination'].str.strip()\n",
    "flight_times['AircraftType'] = flight_times['AircraftType'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## capacity_info\n",
    "At this time, we don't actually use the capacity_info dataset. However, depending on the format of the input data, returning to use of this DF may be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure correct variable types in datasets\n",
    "capacity_info['Subfleet'] = capacity_info['Subfleet'].astype(str)\n",
    "mealsPerFlight_historic['AircraftType'] = mealsPerFlight_historic['AircraftType'].astype(str)\n",
    "input_dataset['AircraftType'] = input_dataset['AircraftType'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns which have dirty data (should be int but include non-numerical values)\n",
    "columns_to_process = ['C', 'F', 'W', 'Y', 'Total']\n",
    "\n",
    "# Convert columns to string\n",
    "capacity_info[columns_to_process] = capacity_info[columns_to_process].astype(str)\n",
    "\n",
    "# Extract only the numeric part before any non-numeric characters occur (most noise is something like \"151(1)\", we want to remove the \"(1)\")\n",
    "for column in columns_to_process:\n",
    "    capacity_info[column] = capacity_info[column].apply(lambda x: re.search(r'\\d+', x).group() if re.search(r'\\d+', x) else '')\n",
    "\n",
    "# Convert columns to integers after fixing them\n",
    "capacity_info[columns_to_process] = capacity_info[columns_to_process].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## airport_codes\n",
    "Here, we create a table of each airport IATA code, the country where the airport is located, and the coordinates of the airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the necessary columns from the dataset\n",
    "airport_codes = airport_codes[[\"iata_code\", 'iso_country', 'coordinates']]\n",
    "\n",
    "# Originally, coordinates are stored as a string like \"latitude, longitude\"\n",
    "# Instead, store them as a tuple for easier access to each attribute\n",
    "airport_codes['coordinates'] = airport_codes['coordinates'].apply(lambda x: tuple(map(float, x.split(', ')))) \n",
    "\n",
    "# Drop rows with missing values in the \"iata_code\" column\n",
    "# There is a lot of data we don't need here, any airport that doesn't have an IATA code is unneccesary\n",
    "# All of the airports in our mealsPerFlight_historic df are identified by their IATA code\n",
    "airport_codes = airport_codes.dropna(subset=[\"iata_code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flight_distances\n",
    "In order to save processing time later, we will pre-compute the distance in miles for each unique origin-destination pair by using the haversine distance based on the latitude and longitude of the origin and destination. This new table of flight distances can be merged with our mealsPerFlight_historic dataset. The flight distance can be used as a feature later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique origins and destinations\n",
    "unique_origins = mealsPerFlight_historic['FlightOrigin'].unique()\n",
    "unique_destinations = mealsPerFlight_historic['FlightDestination'].unique()\n",
    "\n",
    "# Generate all possible combinations\n",
    "unique_combinations = list(itertools.product(unique_origins, unique_destinations))\n",
    "\n",
    "# Merge with airport_codes to get coordinates for origin\n",
    "flight_distances = pd.merge(unique_combinations, airport_codes, left_on='FlightOrigin', right_on='iata_code', how='left')\n",
    "flight_distances.rename(columns={'coordinates': 'OriginCoordinates', 'iso_country': 'OriginCountry'}, inplace=True)\n",
    "\n",
    "# Merge with airport_codes to get coordinates for destination\n",
    "flight_distances = pd.merge(flight_distances, airport_codes, left_on='FlightDestination', right_on='iata_code', how='left')\n",
    "flight_distances.rename(columns={'coordinates': 'DestinationCoordinates', 'iso_country': 'DestinationCountry'}, inplace=True)\n",
    "\n",
    "# Use tuples of latitude and longitude for origin and destination to compute haversine distance for each unique origin-destination pair\n",
    "flight_distances['FlightDistance'] = flight_distances.apply(lambda row: haversine(row['OriginCoordinates'], row['DestinationCoordinates'], unit=Unit.MILES), axis=1)\n",
    "\n",
    "# Splitting the origin coordinates into latitude and longitude\n",
    "flight_distances[['OriginLatitude', 'OriginLongitude']] = flight_distances['OriginCoordinates'].apply(lambda x: pd.Series({'OriginLatitude': x[0], 'OriginLongitude': x[1]}))\n",
    "\n",
    "# Splitting the destination coordinates into lat and long\n",
    "flight_distances[['DestinationLatitude', 'DestinationLongitude']] = flight_distances['DestinationCoordinates'].apply(lambda x: pd.Series({'DestinationLatitude': x[0], 'DestinationLongitude': x[1]}))\n",
    "\n",
    "# Drop now unneccessary columns\n",
    "flight_distances.drop(columns=['iata_code_x', 'iata_code_y', 'OriginCoordinates', 'DestinationCoordinates'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In scope FlightOrigin, MealCode\n",
    "Based on the information provided by Angela on March 3rd, 2024, we must reduce the scope of the dataset. As such, we've created a list for the in scope flight origins and meal codes. Then, we use a function to remove rows that do not fall within the scope of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid flight origins list (as provided by Delta)\n",
    "valid_flight_origins = ['ANC', 'ATL', 'AUS', 'BDL', 'BNA', 'BOS', 'BWI', 'CLT', 'CMH', 'CVG', 'DCA', 'DEN', 'DFW', 'DTW', 'EWR', 'FLL', 'HNL', 'IAD', 'IAH', 'IND', 'JAX', 'JFK', 'KOA', 'LAS', 'LAX', 'LGA', 'MCI', 'MCO', 'MEM', 'MEX', 'MIA', 'MKE', 'MSP', 'MSY', 'OGG', 'ONT', 'ORD', 'PBI', 'PDX', 'PHL', 'PHX', 'PIT', 'PVD', 'RDU', 'RSW', 'SAN', 'SAT', 'SEA', 'SFO', 'SJC', 'SLC', 'SMF', 'SNA', 'STL', 'TPA']\n",
    "\n",
    "# Valid meal codes list (as provided by Delta)\n",
    "valid_meal_codes = ['AVML', 'VLML', 'BLML', 'CHML', 'DBML', 'GFML', 'HNML', 'KSML', 'LFML', 'LSML', 'MOML', 'VGML', 'TDML', 'BBML']\n",
    "\n",
    "# Function to filter the dataframe to only include rows where the FlightOrigin and MealCode fall into the scope of the project\n",
    "def in_scope_filtering(dataframe):\n",
    "    scope_df = dataframe.copy()\n",
    "    scope_df = scope_df[scope_df['FlightOrigin'].isin(valid_flight_origins)] # Filter for flight origins\n",
    "    scope_df = scope_df[scope_df['MealCode'].isin(valid_meal_codes)] # Filter for mealcodes\n",
    "\n",
    "    return scope_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate to ignore EntreeType\n",
    "In the original dataset, there is a row for each meal served for each MealCode in each CabinCode of each flight. However, we want to predict the number of customers we need to serve -- not the number of meals we need to serve to each customer (although, in the aggregate model, we come closer to that in actuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate to number of customers to serve\n",
    "def aggregate_to_passenger_preference(dataframe):\n",
    "    agg_df = dataframe.copy()\n",
    "\n",
    "    # Group by the specified columns and aggregate\n",
    "    agg_df = agg_df.groupby(['FlightDate', 'FlightOrigin', 'FlightDestination', \n",
    "                        'FlightNumber', 'AircraftType', 'MealCode', \n",
    "                        'CabinCode', 'CateredQuantity'], observed=True)['EntreeType'].unique().reset_index()\n",
    "    \n",
    "    agg_df = agg_df.drop(columns=['EntreeType']) # Drop the column we ignored in the aggregation\n",
    "        \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All meals on all flights\n",
    "In the initial mealsPerFlight_historic dataset, in most instances when a meal was not catered for a flight, there was no row to reflect that meal. However, in actuality, if meals are offered on a flight, then all meals are available in all cabins (besides domestic flights, where only first class is served meals).\n",
    "So, we must re-process the initial dataset to ensure that, for all flights, we see all meals for all cabins. In another step, we will filter out cabins and flights which won't actually receive a meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique values for MealCode and CabinCode\n",
    "meal_codes = ['AVML', 'VLML', 'BLML', 'CHML', 'DBML', 'GFML', 'HNML', 'KSML', 'LFML', 'LSML', 'MOML', 'VGML', 'TDML', 'BBML']\n",
    "cabin_codes = ['C', 'F', 'W', 'Y']\n",
    "\n",
    "# Function to ensure that for all flights, there is a row for each possible combination of MealCode and CabinCode\n",
    "def all_meals_all_flights(dataframe):\n",
    "    all_meals_df = dataframe.copy()\n",
    "\n",
    "    # Create a list of all unique combinations of FlightDate, FlightOrigin, FlightDestination, FlightNumber, and AircraftType\n",
    "    flight_combinations = all_meals_df[['FlightDate', 'FlightOrigin', 'FlightDestination', 'FlightNumber', 'AircraftType']].drop_duplicates()\n",
    "\n",
    "    # Create a list of all possible combinations of MealCode and CabinCode\n",
    "    meal_cabin_combinations = pd.DataFrame(list(product(meal_codes, cabin_codes)), columns=['MealCode', 'CabinCode'])\n",
    "\n",
    "    # Merge flight_combinations with meal_cabin_combinations to get all possible combinations\n",
    "    all_combinations = pd.merge(flight_combinations, meal_cabin_combinations, how='cross')\n",
    "\n",
    "    # Merge all_combinations with the original 'all_meals_df' dataframe to fill missing rows and set CateredQuantity to 0 where necessary\n",
    "    all_meals_df = pd.merge(all_combinations, all_meals_df, on=['FlightDate', 'FlightOrigin', 'FlightDestination', 'FlightNumber', 'AircraftType', 'MealCode', 'CabinCode'], how='left')\n",
    "    all_meals_df['CateredQuantity'] = all_meals_df['CateredQuantity'].fillna(0)\n",
    "\n",
    "    return all_meals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country, Distance\n",
    "Merge the input (mealsPerFlight_historic) dataset with the flight_distances dataset we computed at the start. This notably adds the coordinates and total distance of each flight, as well as the origin and destination country. Using the origin and destination countries, we can filter out rows that fall out of the scope of the project (rows that reflect passengers who won't receive meals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_association(dataframe):\n",
    "    dist_df = pd.merge(dataframe, flight_distances, on=['FlightOrigin', 'FlightDestination'], how='left')\n",
    "\n",
    "    # Drop the flights that won't be served meals at any time\n",
    "    # Domestic flights (origin and destination in US) that are under 900 miles only get beverage service\n",
    "    dist_df = dist_df[((dist_df['OriginCountry'] != 'US') | (dist_df['DestinationCountry'] != 'US')) | (dist_df['FlightDistance'] >= 900)]\n",
    "\n",
    "    # We also know that domestic flights that do have meal service ONLY serve meals to first class\n",
    "    dist_df = dist_df[~((dist_df['DestinationCountry'] == 'US') & (dist_df['OriginCountry'] == 'US') & (dist_df['FlightDistance'] >= 900) & (dist_df['CabinCode'] != 'C'))]\n",
    "\n",
    "    # Becuase of the scope of the project, OriginCountry is US for all flights. \n",
    "    # Additionally, we can't use DestinationCountry because of aggregation in later steps\n",
    "    dist_df.drop(columns=['OriginCountry', 'DestinationCountry'], inplace=True) \n",
    "\n",
    "    return dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeOfDay\n",
    "Using the FlightDistance we computed and the DepartTime we merged in, we can ensure that the dataset reflects the \"time of day\" variable from the sample report structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the TimeOfDay column\n",
    "def time_of_day(dataframe):\n",
    "    time_df = dataframe.copy()\n",
    "\n",
    "    # conditions = [\n",
    "    #     (time_df['IsDom'] == 1) & (900 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 1499) & (500 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 945),\n",
    "    #     (time_df['IsDom'] == 1) & (1500 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 2299) & (946 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 1559),\n",
    "    #     (time_df['IsDom'] == 1) & (1500 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 2299) & (1600 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 2059),\n",
    "    #     (time_df['IsDom'] == 1) & ((time_df['FlightDistance'] >= 2300) & ((1600 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) | (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 459))),\n",
    "    #     (time_df['IsInt'] == 1) & (500 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 929),\n",
    "    #     (time_df['IsInt'] == 1) & (930 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 2359)\n",
    "    # ]\n",
    "\n",
    "    conditions = [\n",
    "        (time_df['DomOrInt'] == 'D') & (900 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 1499) & (500 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 945),\n",
    "        (time_df['DomOrInt'] == 'D') & (1500 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 2299) & (946 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 1559),\n",
    "        (time_df['DomOrInt'] == 'D') & (1500 <= time_df['FlightDistance']) & (time_df['FlightDistance'] <= 2299) & (1600 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 2059),\n",
    "        (time_df['DomOrInt'] == 'D') & ((time_df['FlightDistance'] >= 2300) & ((1600 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) | (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 459))),\n",
    "        (time_df['DomOrInt'] == 'I') & (500 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 929),\n",
    "        (time_df['DomOrInt'] == 'I') & (930 <= time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute) & (time_df['DepartTime'].dt.hour * 100 + time_df['DepartTime'].dt.minute <= 2359)\n",
    "    ]\n",
    "\n",
    "    choices = [\"Breakfast\", \"Lunch\", \"Dinner\", \"Dinner\", \"Brunch\", \"Lunch/Dinner\"]\n",
    "\n",
    "    time_df['TimeOfDay'] = np.select(conditions, choices, default=\"Unknown\")\n",
    "\n",
    "    return time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Meals?\n",
    "Creates new columns to one hot encode which meals should be served"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meal_columns(dataframe):\n",
    "    # Copy the dataframe to avoid modifying the original\n",
    "    meals_df = dataframe.copy()\n",
    "    \n",
    "    # Add columns and set default values to 0\n",
    "    meals_df['1stServiceMeal'] = 0\n",
    "    meals_df['PreArrivalMeal'] = 0\n",
    "    meals_df['MidflightMeal'] = 0\n",
    "    \n",
    "    # Compute conditions\n",
    "    is_int_condition = meals_df['DomOrInt'] == 'I'\n",
    "    depart_time_condition = meals_df['DepartTime'].dt.hour.between(5, 23)\n",
    "    flight_distance_condition = meals_df['FlightDistance'] >= 6000\n",
    "    \n",
    "    # Update columns based on conditions\n",
    "    meals_df.loc[is_int_condition & depart_time_condition, '1stServiceMeal'] = 1\n",
    "    meals_df.loc[is_int_condition & depart_time_condition, 'PreArrivalMeal'] = 1\n",
    "    meals_df.loc[is_int_condition & depart_time_condition & flight_distance_condition, 'MidflightMeal'] = 1\n",
    "    \n",
    "    return meals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create date related columns\n",
    "Extract useful features from the FlightDate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_columns(dataframe):\n",
    "    dates_df = dataframe.copy()\n",
    "\n",
    "    dates_df['Year'] = dates_df['FlightDate'].dt.year\n",
    "    dates_df['Month'] = dates_df['FlightDate'].dt.month\n",
    "    dates_df['Day'] = dates_df['FlightDate'].dt.day\n",
    "    # dates_df['DayOfYear'] = dates_df['FlightDate'].dt.dayofyear\n",
    "    dates_df['WeekOfYear'] = dates_df['FlightDate'].dt.isocalendar().week\n",
    "    dates_df['DayOfTheWeek'] = dates_df['FlightDate'].dt.day_name()\n",
    "\n",
    "    return dates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dist from Holidays\n",
    "Using the 'holidays' package in Python, we can find the number of days away from relevant holidays for each flight.\n",
    "Do some precomputing and merging based on the unique FlightDate values in the dataset.\n",
    "For the most part, the holidays seemed to correlate with each other. So, we kept the two most notable holidays which had high feature importance and which also are based on different calendars than the US calendar (and thus store different information than what we can extract from FlightDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of holiday names\n",
    "holiday_names = [\n",
    "    'Chinese New Year (Spring Festival)',\n",
    "    'Pesach'\n",
    "]\n",
    "\n",
    "# Function to add features to dataset which indicate how far each flight is from the occurence of a certain holiday within that calendar year\n",
    "# So, Jan 1st would be -360 days from Christmas, because it's based on the calendar year\n",
    "def add_holiday_distance(dataframe):\n",
    "    unique_years = pd.to_datetime(dataframe['FlightDate']).dt.year.unique().tolist()\n",
    "    unique_years = sorted(unique_years)\n",
    "\n",
    "    # Create DataFrame\n",
    "    holiday_df = pd.DataFrame(index=holiday_names, columns=unique_years)\n",
    "\n",
    "    us_holidays = holidays.US()\n",
    "    il_holidays = holidays.IL(language='en_US')\n",
    "    cn_holidays = holidays.CN(language='en_US')\n",
    "\n",
    "    for year in holiday_df.columns:\n",
    "        # Generate date range for the current year\n",
    "        start_date = pd.Timestamp(year, 1, 1)\n",
    "        end_date = pd.Timestamp(year, 12, 31)\n",
    "        date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "        # Iterate over each day in the date range\n",
    "        for date in date_range:\n",
    "            il_holiday_name = il_holidays.get(date)\n",
    "            if il_holiday_name in holiday_df.index:\n",
    "                holiday_df.at[il_holiday_name, year] = date\n",
    "\n",
    "            cn_holiday_name = cn_holidays.get(date)\n",
    "            if cn_holiday_name in holiday_df.index:\n",
    "                holiday_df.at[cn_holiday_name, year] = date\n",
    "\n",
    "    unique_dates = dataframe['FlightDate'].unique().tolist()\n",
    "\n",
    "    # Create an empty DataFrame with unique_dates as index\n",
    "    reference_df = pd.DataFrame(index=unique_dates)\n",
    "\n",
    "    # iterate over each date in the dataset\n",
    "    for date in reference_df.index:\n",
    "        year = date.year\n",
    "        # Iterate over each holiday\n",
    "        for holiday in holiday_df.index:\n",
    "            holiday_date = holiday_df.at[holiday, year]\n",
    "            # Check if holiday_date is not NaN (ie, a valid date)\n",
    "            if not pd.isnull(holiday_date):\n",
    "                holiday_cleaned = holiday.strip().translate(str.maketrans('', '', string.punctuation)).replace(\" \", \"\")\n",
    "                reference_df.at[date, \"DistFrom\" + holiday_cleaned] = (date - holiday_date).days\n",
    "\n",
    "    reference_df.index = pd.to_datetime(reference_df.index)\n",
    "\n",
    "    # Merge dataframe and reference_df on the FlightDate column and the index\n",
    "    merged_df = pd.merge(dataframe, reference_df, left_on='FlightDate', right_index=True, how='left')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "CabinCode and DayOfTheWeek have natural order to them, so it's better to perform ordinal encoding rather than label or one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store mappings\n",
    "cabin_mapping = {'C': 1, 'F': 2, 'W': 3, 'Y': 4}\n",
    "days_mapping = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "\n",
    "# Function to perform ordinal encoding on applicable features\n",
    "def ordinal_encoding(dataframe):\n",
    "    ord_df = dataframe.copy()\n",
    "\n",
    "    ord_df['CabinCode'] = ord_df['CabinCode'].map(cabin_mapping)\n",
    "    ord_df['DayOfTheWeek'] = ord_df['DayOfTheWeek'].map(days_mapping)\n",
    "\n",
    "    return ord_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "For features with no natural order, we label encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "# Simple function to label encode all categorical variables\n",
    "def label_encoding(dataframe):\n",
    "    lab_df = dataframe.copy()\n",
    "\n",
    "    for column in lab_df.columns:\n",
    "        if not lab_df[column].dtype.kind in ['i', 'f', 'M']:  # Check if column is not numeric\n",
    "            le = LabelEncoder()\n",
    "            lab_df[column] = le.fit_transform(lab_df[column])\n",
    "            label_encoders[column] = le\n",
    "    \n",
    "    return lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_input(dataframe):\n",
    "    lab_df = dataframe.copy()\n",
    "\n",
    "    for column, le in label_encoders.items():\n",
    "            if column in lab_df.columns:\n",
    "                lab_df[column] = le.transform(lab_df[column])\n",
    "    \n",
    "    return lab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Capacity\n",
    "Two functions in here; which to use depends on the features available in the input data. Currently, for the sake of our code and experiments, we use the simpler function which does a merge to bring the time and capacity data into our primary dataset\n",
    "\n",
    "NOTE That some flights do not seem to be able to retrieve their flight capacity from either dataset. Thus, the fleet key dataset we provided may need some manual adjustmenets to best reflect the entire Delta fleet and the capacity of each cabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_capacity(dataframe):\n",
    "    ppl_df = pd.merge(dataframe, flight_times, on=['FlightDate', 'FlightNumber', 'FlightOrigin', 'FlightDestination', 'AircraftType'], how='left')\n",
    "\n",
    "    # Drop rows where we are missing data\n",
    "    # Small subset of rows thankfully\n",
    "    ppl_df.dropna(inplace=True)\n",
    "\n",
    "    return ppl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to select capacity based on CabinCode\n",
    "def get_capacity(row):\n",
    "    cabin_code = row['CabinCode']\n",
    "    if cabin_code == 'C':\n",
    "        return row['C']\n",
    "    elif cabin_code == 'F':\n",
    "        return row['F']\n",
    "    elif cabin_code == 'W':\n",
    "        return row['W']\n",
    "    elif cabin_code == 'Y':\n",
    "        return row['Y']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to use a supplemental df (capacity_info) to find the capacity of each cabin of each flight based on SubFleet\n",
    "# requires you to load capacity_info, which has data on the capacity of each cabin of each subfleet of plane\n",
    "def people_capacity_input(dataframe):\n",
    "    ppl_df = pd.merge(dataframe, capacity_info, left_on='AircraftType', right_on='Subfleet', how='left')\n",
    "\n",
    "    # Apply the function to create the new PeopleCapacity column\n",
    "    ppl_df['PeopleCapacity'] = ppl_df.apply(get_capacity, axis=1)\n",
    "\n",
    "    ppl_df.drop(['Aircraft Model', 'Subfleet', 'Configuration', 'Count', 'C', 'F', 'W', 'Y', 'Total'], axis=1, inplace=True)\n",
    "\n",
    "    # ppl_df = ppl_df[ppl_df['PeopleCapacity'] != 0].dropna(subset=['PeopleCapacity'])\n",
    "\n",
    "    return ppl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Categorical Columns (For XGBoost Model)\n",
    "Function to automatically assign columns as \"category\" type. Useful when running an XGBoost model which can use categorical features. XGBoost performance is improved when using the experimental feature to use categorical columns without needing encoding. It also makes error analysis simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must run this before running create_dummies()\n",
    "def def_cat_cols(dataframe):\n",
    "    cat_df = dataframe.copy()\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = cat_df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "    # Label encode categorical columns\n",
    "    for col in categorical_columns:\n",
    "        cat_df[col] = cat_df[col].astype('category')\n",
    "\n",
    "    return cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables\n",
    "If dummy variables are preferred over label encoding, this function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dummy variables for all categorical columns\n",
    "# REQUIRES you to run def_cat_cols() first in order to ensure that categorical columns use the correct datatype\n",
    "def create_dummies(dataframe):\n",
    "    dummy_df = dataframe.copy()    \n",
    "    categorical_columns = dummy_df.select_dtypes(include=['category']).columns.tolist()\n",
    "    dummy_df = pd.get_dummies(dummy_df, columns=categorical_columns)    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale\n",
    "Some of our model experimentation required scaling, this function is to make the scaling process simpler to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will scale your already created train, test, and validation sets\n",
    "def scale_data(train, test, val):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    val_scaled = scaler.transform(val)\n",
    "\n",
    "    return train_scaled, test_scaled, val_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agg to origin-day\n",
    "In our most recent iterations of the modeling code, we aggregate to the MealCode-CabinCode-Flight-TimeOfDay-Day level. This function simply runs the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_day_agg(dataframe):\n",
    "    # Grouping by the desired columns and aggregating\n",
    "    agg_df = dataframe.groupby(['FlightDate', 'FlightOrigin', 'CabinCode', 'MealCode', 'DomOrInt', 'OriginLatitude', 'OriginLongitude', 'Year', 'Month', 'Day', 'WeekOfYear', 'DayOfTheWeek', 'DistFromChineseNewYearSpringFestival', 'DistFromPesach', 'TimeOfDay', '1stServiceMeal', 'PreArrivalMeal', 'MidflightMeal'], observed=True)\\\n",
    "        .agg(\n",
    "        NumFlights=('FlightNumber', 'nunique'),  \n",
    "        TotalCateredQuantity=('CateredQuantity', 'sum'),  \n",
    "        TotalDistance=('FlightDistance', 'sum'),\n",
    "        TotalCapacity=('PassengerCapacity', 'sum'),\n",
    "    ).reset_index()  # Resetting index to flatten the DataFrame\n",
    "        \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_day_agg_input(dataframe):\n",
    "    # Grouping by the desired columns and aggregating\n",
    "    agg_df = dataframe.groupby(['FlightDate', 'FlightOrigin', 'CabinCode', 'MealCode', 'DomOrInt', 'OriginLatitude', 'OriginLongitude', 'Year', 'Month', 'Day', 'WeekOfYear', 'DayOfTheWeek', 'DistFromChineseNewYearSpringFestival', 'DistFromPesach', 'TimeOfDay', '1stServiceMeal', 'PreArrivalMeal', 'MidflightMeal'], observed=True)\\\n",
    "        .agg(\n",
    "        NumFlights=('FlightNumber', 'nunique'),  \n",
    "        TotalDistance=('FlightDistance', 'sum'),\n",
    "        TotalCapacity=('PassengerCapacity', 'sum'),\n",
    "    ).reset_index()  # Resetting index to flatten the DataFrame\n",
    "        \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "For error analysis, this simply decodes the various features of our dataframes based on our encoding functions used to create the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_dataframe(dataframe):\n",
    "    decoded_dataframe = dataframe.copy()\n",
    "\n",
    "    for column, le in label_encoders.items():\n",
    "            if column in decoded_dataframe.columns:\n",
    "                decoded_dataframe[column] = le.inverse_transform(decoded_dataframe[column])\n",
    "    \n",
    "    decoded_dataframe['CabinCode'] = decoded_dataframe['CabinCode'].map({v: k for k, v in cabin_mapping.items()})\n",
    "    decoded_dataframe['DayOfTheWeek'] = decoded_dataframe['DayOfTheWeek'].map({v: k for k, v in days_mapping.items()})\n",
    "    \n",
    "    return decoded_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isHub?\n",
    "It was noted by Venkatesh at the GBAC that there might be a difference in ordering trends at Delta hub airports. Based on information from the Delta website, we have identified the hub airports. However, we have found the 'isHub' feature to be of low feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHub(dataframe):\n",
    "    hub_df = dataframe.copy()\n",
    "\n",
    "    # List of IATA codes\n",
    "    delta_hubs = ['AMS', 'ATL', 'BOG', 'BOS', 'DTW', 'LIM', 'LHR', 'LAX', 'MEX', 'MSP', 'JFK', 'LGA', \n",
    "                'CDG', 'SLC', 'SCL', 'GRU', 'SEA', 'ICN', 'NRT', 'HND']\n",
    "\n",
    "    # Create a new column \"IsHub\" initialized with zeros\n",
    "    hub_df['IsHub'] = 0\n",
    "\n",
    "    # Set IsHub to 1 for rows where FlightOrigin is in the list of IATA codes\n",
    "    hub_df.loc[hub_df['FlightOrigin'].isin(delta_hubs), 'IsHub'] = 1\n",
    "    \n",
    "    return hub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historic Data Pipeline\n",
    "Here is where we run the current data processing pipeline as we have found to be most effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_historic_data = (mealsPerFlight_historic.pipe(in_scope_filtering)\n",
    "                               .pipe(aggregate_to_passenger_preference) # In original dataset, customers are represented for every meal they are served. This aggregates down to the number of customer preferences, not the number of meals served.\n",
    "                               .pipe(all_meals_all_flights) # Ensure each meal is represented in each cabin of each flight. \n",
    "                               .pipe(people_capacity) # Deletes any rows where we can't find a capacity (NAN), or when capacity is 0\n",
    "                               .pipe(country_association) # Get country association, flight distances, and filter data based on flights which actually receive meals\n",
    "                               .pipe(time_of_day) # Get the time of day and other time-based features which dictates which meals are served\n",
    "                               .pipe(add_meal_columns) # For int'l flights, keep tracks of which meals will be received\n",
    "                               .pipe(date_columns) # Create features from FlightDate\n",
    "                               .pipe(add_holiday_distance) # Create distance from Passover and from Chinese New Year as features\n",
    "                               .pipe(origin_day_agg) # Aggregate to MealCode-CabinCode-Flight-TimeOfDay-Date level\n",
    "                               .pipe(isHub) # Identify if the airport is a Delta hub\n",
    "                               .pipe(ordinal_encoding) # perform ordinal encoding for CabinCode, DayOfTheWeek\n",
    "                               .pipe(label_encoding) # perform label encoding for non-ordinal and non-numerical columns\n",
    "                               .sort_values(by=['FlightDate', 'FlightOrigin', 'CabinCode', 'MealCode']) # sort for a date-based train-test split\n",
    "                               )\n",
    "\n",
    "X_train = processed_historic_data.drop(columns=['TotalCateredQuantity', 'FlightDate'])\n",
    "y_train = processed_historic_data['TotalCateredQuantity']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_input_data = (input_dataset\n",
    "                               .pipe(all_meals_all_flights) # Ensure each meal is represented in each cabin of each flight. \n",
    "                               .pipe(people_capacity_input) # Deletes any rows where we can't find a capacity (NAN), or when capacity is 0\n",
    "                               .pipe(country_association) # Get country association, flight distances, and filter data based on flights which actually receive meals\n",
    "                               .pipe(time_of_day) # Get the time of day and other time-based features which dictates which meals are served\n",
    "                               .pipe(add_meal_columns) # For int'l flights, keep tracks of which meals will be received\n",
    "                               .pipe(date_columns) # Create features from FlightDate\n",
    "                               .pipe(add_holiday_distance) # Create distance from Passover and from Chinese New Year as features\n",
    "                               .pipe(origin_day_agg_input) # Aggregate to MealCode-CabinCode-Flight-TimeOfDay-Date level\n",
    "                               .pipe(isHub) # Identify if the airport is a Delta hub\n",
    "                               .pipe(ordinal_encoding) # perform ordinal encoding for CabinCode, DayOfTheWeek\n",
    "                               .pipe(label_encoding_input) # perform label encoding for non-ordinal and non-numerical columns\n",
    "                               .sort_values(by=['FlightDate', 'FlightOrigin', 'CabinCode', 'MealCode']) # sort for a date-based train-test split\n",
    "                               .drop(columns=['FlightDate']) # Drop FlightDate\n",
    "                               )\n",
    "\n",
    "# Create a decoded X dataset for easier output later\n",
    "processed_input_decoded = decode_dataframe(processed_input_data)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Time series feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SARIMA model\n",
    "model = SARIMAX(y_train, order=(1, 1, 0), seasonal_order=(1, 0, 0, 12)) \n",
    "# Fit the model\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# Make predictions\n",
    "# Predict for X_train\n",
    "train_predictions = model_fit.predict(start=0, end=len(X_train)-1)\n",
    "# Predict for processed_input_data\n",
    "input_predictions = model_fit.predict(start=len(X_train), end=len(X_train)+len(processed_input_data)-1)\n",
    "\n",
    "# Add train_predictions as a new column to X_train\n",
    "X_train['ARIMAForecast'] = train_predictions\n",
    "\n",
    "# Add test_predictions as a new column to processed_input_data\n",
    "processed_input_data['ARIMAForecast'] = input_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM\n",
    "When you use the LGBM model for prediction, it provides balanced results. However, while in real life we're limited to ordering an integer number of meals, the model is a regressor and outputs a float. So, we must round. This rounding unfortunately does result in a greater frequency of stockouts compared to overstocking. \n",
    "\n",
    "Fortunately, we have found that adding a small value to all predictions -- some number between 0.15-0.25 seems to produce the best results -- can balance the overall error of the model with reducing stockouts. You can adjust this parameter in 'epsilon' to achieve the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adjust the value of epsilon to fine tune the over vs. underprediction of the model\n",
    "epsilon = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(verbosity=-1)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "joblib.dump(lgb_model, 'lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk if desired\n",
    "# lgb_model = joblib.load('lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(lgb_model.predict(processed_input_data) + epsilon)\n",
    "\n",
    "# Create a DataFrame to store the results more neatly\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': pd.to_datetime(processed_input_decoded[['Year', 'Month', 'Day']]),\n",
    "    'ISO Dep Week': processed_input_decoded['WeekOfYear'],\n",
    "    'Origin': processed_input_decoded['FlightOrigin'],\n",
    "    'Cabin': processed_input_decoded['CabinCode'],\n",
    "    'SPML Code': processed_input_decoded['MealCode'],\n",
    "    'Domestic/International/TCON': processed_input_decoded['DomOrInt'],\n",
    "    'Breakfast/Lunch/Dinner/PreArrival, etc.': processed_input_decoded['TimeOfDay'],\n",
    "    '1stServiceMeal': processed_input_decoded['1stServiceMeal'],\n",
    "    'PreArrivalMeal': processed_input_decoded['PreArrivalMeal'],\n",
    "    'MidflightMeal': processed_input_decoded['MidflightMeal'],  \n",
    "    'Predicted Demand (Meals)': y_pred\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the duplicated rows\n",
    "duplicated_rows = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in results_df.iterrows():\n",
    "    if (row['1stServiceMeal'] == 1) or (row['PreArrivalMeal'] == 1) or (row['MidflightMeal'] == 1):\n",
    "        # Check the values of the three columns\n",
    "        if row['1stServiceMeal'] == 1:\n",
    "            # Duplicate the row and add the meal type\n",
    "            new_row = row.copy()\n",
    "            new_row['MealTime'] = '1st Service Meal'\n",
    "            duplicated_rows.append(new_row)\n",
    "        if row['PreArrivalMeal'] == 1:\n",
    "            # Duplicate the row and add the meal type\n",
    "            new_row = row.copy()\n",
    "            new_row['MealTime'] = 'Pre-Arrival Meal'\n",
    "            duplicated_rows.append(new_row)\n",
    "        if row['MidflightMeal'] == 1:\n",
    "            # Duplicate the row and add the meal type\n",
    "            new_row = row.copy()\n",
    "            new_row['MealTime'] = 'Midflight Meal'\n",
    "            duplicated_rows.append(new_row)\n",
    "    elif (row['1stServiceMeal'] == 0) and (row['PreArrivalMeal'] == 0) or (row['MidflightMeal'] == 0):\n",
    "        duplicated_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame with the duplicated rows\n",
    "transformed_df = pd.DataFrame(duplicated_rows)\n",
    "transformed_df['MealTime'] = transformed_df['MealTime'].fillna('').copy()\n",
    "transformed_df = transformed_df.drop(columns=['1stServiceMeal', 'PreArrivalMeal', 'MidflightMeal']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['Date'] = transformed_df['Date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.now().strftime('%m-%d-%Y')\n",
    "\n",
    "# Create the 'Output' directory if it doesn't exist\n",
    "output_dir = f'Output (Generated {today_date})'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Group the DataFrame by 'Date' and 'Origin'\n",
    "grouped = transformed_df.groupby(['Date', 'Origin'])\n",
    "\n",
    "# Iterate over each group\n",
    "for (date, origin), group_df in grouped:\n",
    "    # Create a folder for each date if it doesn't exist\n",
    "    date_folder = os.path.join(output_dir, date)\n",
    "    if not os.path.exists(date_folder):\n",
    "        os.makedirs(date_folder)\n",
    "\n",
    "    # Sort the group DataFrame by the specified columns\n",
    "    sorted_group = group_df.sort_values(by=['Breakfast/Lunch/Dinner/PreArrival, etc.', 'Cabin', 'SPML Code'])\n",
    "\n",
    "    # Create a filename for the Excel file\n",
    "    filename = f\"{origin}_{date}.xlsx\"\n",
    "    filepath = os.path.join(date_folder, filename)\n",
    "\n",
    "    # Write the sorted DataFrame to an Excel file\n",
    "    with pd.ExcelWriter(filepath, engine='xlsxwriter') as writer:\n",
    "        sorted_group.to_excel(writer, index=False, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "elapsed_minutes = int(elapsed_time_seconds // 60)\n",
    "elapsed_seconds = int(elapsed_time_seconds % 60)\n",
    "\n",
    "print(\"Elapsed time:\", elapsed_minutes, \"minutes and\", elapsed_seconds, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
